<!DOCTYPE html>
<html>
  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <link href='https://fonts.googleapis.com/css?family=Architects+Daughter' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/pygment_trac.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/print.css" media="print">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

    <title>Anon by pkholland</title>
  </head>

  <body>
    <header>
      <div class="inner">
        <h1>Anon</h1>
        <h2>Experiments in Web Services Design</h2>
        <a href="https://github.com/pkholland/anon" class="button"><small>View project on</small> GitHub</a>
      </div>
    </header>

    <div id="content-wrapper">
      <div class="inner clearfix">
        <section id="main-content">

<h1>
<a id="tech-overview" class="anchor" href="#tech-overview" aria-hidden="true"><span class="octicon octicon-link"></span></a>Anon Technical Overview</h2>

<ul>Table of Contents:
<li><a href="#io_concur">Fibers and I/O Driven Concurency</a></li>
<li>EAGAIN Propogation</li>
<li>The look of the API</li>
</ul>

<h2>
<a id="io_concur" class="anchor" href="#io_concur" aria-hidden="true"><span class="octicon octicon-link"></span></a>Fibers and I/O Driven Concurrency
</h2>

<p>
The story here starts at linux's <b>epoll</b> api and how it supports efficient delivery of i/o
notification. In the <b>epoll</b> model an application can register a set of file descriptors
with the kernel, and with each one specify two pieces of data:

<ul>
<li>What i/o <em>events</em> it wants to be notified about</li>
<li>A pointer that it wants to be given when an i/o event occurs for that file descriptor</li>
</ul>

Linux uses file descriptors to represent lots of different things, but from a web services perspective
the most important and typical one is that network sockets are represented with file descriptors.  The
idea of an i/o <em>event</em> is something like the point in time when a socket becomes readable without blocking
-- that is, when data has arrived from the other computer such that a call to <b>recv</b> will return
something without blocking the calling thread.  That is the <b>EPOLLIN</b> event.
</p>

<p>
A reasonable picture of what is going on with <b>epoll</b> is here:
</p>

<p><img src="epoll.svg?raw=true" alt="Linux epoll picture"></p>

<p>
Because linux will store pointers for you associated with each file descriptor, and you can use these
pointers any way you want, like this example you can use them to store the addresses of C++ class
instances. These can implement the pieces of code you want to run whenever it is possible to read from,
or write to, a file <em>without blocking</em>.
</p>

<p>Consider the following pseudo-code:</p>

<div class="highlight highlight-C++"><pre>
<span class="c1">// runs forever...</span>
<span class="kt">void</span> <span class="nf">epoll_loop</span>()
{
  <span class="k">while</span> (<span class="kc">true</span>) {
    the_epoll_result = <span class="nf">epoll_wait</span>();
    the_object = (<span class="kc">base_class</span>*)the_epoll_result;
    the_object->handle_io();
  }
}
</pre></div>

<p>
<b>epoll_wait</b> is the linux api to the epoll system that waits until there is i/o possible on at
least one of the registered file descriptors. When there is it returns information about one such file descriptor.
It isn't being shown correctly in this code example (the parameters are wrong), but in spirit it operates like
this example shows.  In this sort of model, if <b>epoll_wait</b> comes back with a 'result' that
represents file descriptor 48 in the picture above, then a simple virtual function call to <b>handle_io</b>
can get us to the code that knows what to do when <b>another.server.com</b> has sent information to us that we can
read now without blocking.
</p>

</p>
In a number of ways linux's <b>epoll_wait</b> acts like both <b>poll</b> and <b>select</b>
except for a few important differences.  First, and most importantly, with epoll you don't have to send the
set of file descriptors you are interested in each time you call the function as you do with both <b>poll</b>
and <b>select</b>.  This means that the kernel doesn't have to read the set you are telling it on each call and
figure out what to do with each of them.  This can make a significant difference when you have a lot of file
descriptors that you are interested in - as is frequently the case for a web service that is under heavy load.
It's also more convenient to let the kernel associate the file descriptors and pointers for you, than you
having to do it by hand.
</p>

<p>
A last important difference that is largely a consequence of the first one (not needing to specify
the file set on each call), is that it is now fairly straight forward to have multiple threads all waiting for
epoll i/o notification on the same file set.  This can further reduce the processing time needed to go from
event notification to your handling code in any case where you have multi-threaded event handling code.
In anon's epoll model, it calls <b>epoll_wait</b> on the same file set concurrently from multiple threads.
It then dispatches to the handling code directly using function calls as is shown in the code snippet
above.  These handlers simply execute in the OS thread that recieved the notification from <b>epoll_wait</b>.
</p>

<h3>
<a id="callback_design" class="anchor" href="#callback_deskgn" aria-hidden="true"><span class="octicon octicon-link"></span></a>Callback Oriented Design
</h3>

<p>
A significant drawback to this sort of epoll code is that while it is very efficient at notifying you
that you can read some data from a socket, it can't tell you whether you can read <em>all</em> the data
you might be interested in.  It could be that a server has responded with only some of the information it
is going to send you, and you are likely to need to start reading what it has sent so far to even determine
whether it has sent you a complete reply or not.
</p>

<p>
Consider the case where epoll tells you that you can read some data from the <b>another.server.com</b> socket and
you begin running your code to parse the http headers you expect to be at the start of this message.  It could
be that part way through parsing them you get to the end of the data that is currently available to read on that
socket. If this happens your parser has only two realistic choices. It can be designed to use a <em>blocking</em>
socket, in which case the next <b>read</b> call on that socket will block the calling thread until the next network
packet shows up from that server. Or, it can use a <em>non-blocking</em> socket, and then be written to detect the
EAGAIN state. But in that case you need to write your http parser in such a way that it can be suspended and later
resumed when the packet does show up and you get the next epoll notification for it.
</p>

<p>
Each of these has drawbacks.  A common pattern found in code that attempts to always be <em>non-blocking</em>
is the one used by <b>libevent</b> and is the second of the two cases above.  This is sometimes called a
Callback Oriented Design because of its heavy use of callbacks.  The overall design features the idea that
i/o processing can take a long time, so you start by initiating it.  When you do this you provide a
callback function that will get called when the i/o operation completes.  But this sort of design
tends to interrupt the linear flow of source code because the idea of <em>what happens next</em> can't 
easily be specified as the next line in the source code.  In a Callback Oriented Design, the thing
that happens next is frequently specified in some other function.
</p>

<h3>
<a id="fibers" class="anchor" href="#fibers" aria-hidden="true"><span class="octicon octicon-link"></span></a>Fibers and how they help
</h3>

<p>
Fibers are sometimes known as <em>user level threads</em>.  They are supported by many different operation
systems, including linux.  The typical idea behind this is that they are similar to OS threads.  They each
have a stack and register state.  You can make function calls, return from them and all the other "normal"
things.  But unlinke normal operating system threads, they requrire an explicit call to switch from running
one fiber to another.  For a whole host of reasons, switching fiber contexts by calling an API is almost
always faster than the OS switching thread contexts.  But one common reason for them being faster is that
you frequently don't have a particularly good strategy for deciding what fiber to run when.  A good OS thread
scheduling algorithm ensures that all threads are run at reasonable times.  A fiber scheduling algorithm that
attempted this sort of fairness would lose a great deal of its performance advantage over OS thread switching.
</p>

<p>
But for anon, we have a perfect use case for fibers.  In anon, we can continue to execute in a single fiber
until that fiber runs out of i/o.  For example, a fiber can continue to execute and do whatever its logic
is designed for until it gets to the next point of wanting to read something from a socket that has no more
data.  That EAGAIN condition can be used to signify an appropirate time to switch fibers since some other
fiber in the system might be at a point where there is data available for it to read or write.
</p>

<p>
A core piece of anon is that when a fiber hits an EAGAIN condition attempting to perform i/o, it does a
fiber switch back to the point where it calls <b>epoll_wait</b>.  This allows the OS threads that run these
fibers to remain busy executing interesting logic as long as there are sockets that are either ready to
be read from or written to.
</p>

<p>
Anon's fiber model permits code that is structured something like:
</p>

<div class="highlight highlight-C++"><pre>
<span class="c1">// appears to run until the end of the message.
// but, in fact switches out to other fibers if
// this socket runs out of data and others have
// available data</span>
<span class="kt">void</span> <span class="nf">parse_message</span>()
{
  <span class="k">while</span> (token = <span class="nf">read_next_token</span>()) {
    <span class="nf">do_something_with</span>(token);
  }
}
</pre></div>

<p>
In anon, <b>read_next_token</b> does a <em>fiber friendly read</em>, which means that as soon as there are no bytes
to be read it does a fiber-switch back to the <b>epoll_wait</b> code - allowing this OS thread to work on
whatever sockets do have available data.  When this, or any other OS thread sees that there is again data
for this socket (<b>epoll_wait</b> returns a result for this socket), it fiber-switches back to the point
where this fiber previously switched out.  So from a source code perspective it looks like linear flow the
way that thread-blocking operations do.  In the example above, the fiber is simply suspended inside of
<b>read_next_token</b> until that function can read an entire token.
</p>

<p>
This ends up meaning that anon's fiber scheduling rules are derived from epoll's i/o event delivery mechanism.
Anon implements an <b>i/o driven concurrency model.</b>
</p>

<h3>
<a id="fiber_timing" class="anchor" href="#fiber_timing" aria-hidden="true"><span class="octicon octicon-link"></span></a>Does that actually help?
</h3>

<p>
Like anything, it depends on what you measure against.  But you can do comparisons to similar OS thread switching
by writing an application that does the following:
</p>

<ul>
<li>In one thread, <b>accept</b> socket connections and create "handlers" for each</li>
<li>In a second thread, make many <b>connect</b> calls to the listening socket of the first thread</li>
</ul>

<p>
Now, consider a very simple protocol where the sender (second thread) just sends a single byte.
When the handler sees a byte it echos that byte back.  After the sender sends its byte it
attempts to read this echoed byte from the handler.  To compare the fiber solution to a normal OS
thread solution we want to look at the difference between the first thread creating new OS
threads for each new handler vs. creating new fibers.  Since the syntax we can use once we are inside
of a fiber is similar to what we can do inside of a thread, we can just say that in both cases the handlers
look something like:
</p>

<div class="highlight highlight-C++"><pre>
<span class="c1">// protocol handler</span>
<span class="kt">void</span> <span class="nf">handle_message</span>(<span class="k">pipe</span>& p)
{
  <span class="k">while</span> (byte = p.<span class="nf">read</span>()) {
    p.<span class="nf">write</span>(byte);
  }
}
</pre></div>

<p>
In an OS thread case, <b>pipe</b> is a type where <b>read</b> and <b>write</b> are directly
mapped to <b>recv</b> and <b>send</b> and the underlying file descriptor is set <em>blocking</em>.
For the fiber case the file descriptor is <em>non-blocking</em> and the methods are mapped to versions of
these calls that check for <b>EAGAIN</b>.  When they detect it they fiber-switch back to the
<b>epoll_wait</b> code.  In both cases, when the caller sends a zero byte the protocol is over and
the routine returns, terminating the OS thread or fiber.
</p>

<p>
In the OS thread case the code that calls <b>accept</b> looks something like:
</p>

<div class="highlight highlight-C++"><pre>
<span class="c1">// accept loop</span>
<span class="kt">void</span> <span class="nf">accept_loop</span>(<span class="k">int</span> listening_socket)
{
  <span class="k">while</span> (<span class="kc">true</span>) {
    <span class="kt">int</span> new_connection = <span class="nf">accept</span>(listening_socket, <span class="m">0</span>, <span class="m">0</span>);
    std::thread(std::bind(<span class="nf">handle_message</span>, new_connection).<span class="nf">detach</span>();
  }
}
</pre></div>

<p>
where you (incorrectly) have to assume that there is some way to automatically convert the new_connection
int (file descriptor) into a non-const <b>pipe</b>&.  Real C++ syntax wouldn't permit this.  But what is
important here is to point out that each call to <b>accept</b> results in a new OS thread running
<b>handle_message</b> with that one socket.
</p>

<p>
The fiber equivalent that we want to compare to is identical except that:
</p>

<ul>
<li>Its <b>handle_message</b> uses a <b>pipe</b> type that does fiber-friendly reads and writes</li>
<li>Its <b>accept_loop</b> creates fibers running <b>handle_message</b> instead of std::threads</li>
</ul>

<p>
With these two different versions of a server, we can write a client that looks something like:
</p>

<div class="highlight highlight-C++"><pre>
<span class="kt">void</span> <span class="nf">client</span>()
{
  <span class="kt">int</span> socks[400];
  <span class="k">for</span> (<span class="kt">int</span> i=0; i<400; i++)
    socks[i] = <span class="nf">connect</span>( ... to the server ... );
    
  <span class="k">for</span> (<span class="kt">int</span> msg=0; i<1000; msg++) {
  
    <span class="c1">// first send one message to each of the sockets</span>
    <span class="k">for</span> (<span class="kt">int</span> i=0; i<400; msg++)
      <span class="nf">send_one_byte</span>(socks[i]);
      
    <span class="c1">// now loop through and read all the responses</span>
    <span class="k">for</span> (<span class="kt">int</span> i=0; i<400; msg++)
      <span class="nf">read_one_byte</span>(socks[i]);
  }
}
</pre></div>

<p>
The client app is being careful to force thread or fiber context switching in the server by writing
one message to each of the connections before entering the phase where it reads replies.  This makes
sure that the server isn't running in mode where it reads long sequences of messages on a single connection
and just returns them without needing to switch to handling other connections.  That sort of "do a small
amount of work on one connection and then move on to the next" behavior is typical in a real world
web service, and is exactly what we want to measure.  What we want now is to put a timer around the
second <b>for</b> loop and compare the timings for the two different kinds of servers.
</p>

<h3>
<a id="fiber_timing_data" class="anchor" href="#fiber_timing_data" aria-hidden="true"><span class="octicon octicon-link"></span></a>Some Actual Timing Data
</h3>

<p>
Anon's test code contains this exact sort of testing, allowing you to make this comparison on whatever
machine configuration you want to test.  The data below was run on an Ubuntu 14.04, 4 CPU core VM. This
ran inside of VMWare running in a 2.3 GHz Intel Mac Pro laptop.  The fiber switching code is designed to
run as many OS threads as there are CPU cores.  So while the <b>client</b> application causes the fiber
server to create 400 fibers, all of these fibers are run on 4 OS threads that are calling <b>epoll_wait</b>.
The OS thread server creates 400 OS threads in response to the <b>client</b> app.
</p>

        </section>

        <aside id="sidebar">
          <a href="https://github.com/pkholland/anon/zipball/master" class="button">
            <small>Download</small>
            .zip file
          </a>
          <a href="https://github.com/pkholland/anon/tarball/master" class="button">
            <small>Download</small>
            .tar.gz file
          </a>

          <p class="repo-owner"><a href="https://github.com/pkholland/anon"></a> is maintained by <a href="https://github.com/pkholland">pkholland</a>.</p>

          <p>This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the Architect theme by <a href="https://twitter.com/jasonlong">Jason Long</a>.</p>
        </aside>
      </div>
    </div>

  
  </body>
</html>

