<!DOCTYPE html>
<html>
  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <link href='https://fonts.googleapis.com/css?family=Architects+Daughter' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/pygment_trac.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/print.css" media="print">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

    <title>Anon by pkholland</title>
  </head>

  <body>
    <header>
      <div class="inner">
        <h1>Anon</h1>
        <h2>Experiments in Web Services Design</h2>
        <a href="https://github.com/pkholland/anon" class="button"><small>View project on</small> GitHub</a>
      </div>
    </header>

    <div id="content-wrapper">
      <div class="inner clearfix">
        <section id="main-content">

<h1>
<a id="tech-overview" class="anchor" href="#tech-overview" aria-hidden="true"><span class="octicon octicon-link"></span></a>Anon Technical Overview</h2>

<ul>Table of Contents:
<li><a href="#io_concur">Fibers and I/O Driven Concurency</a></li>
<li>EAGAIN Propogation</li>
<li>The look of the API</li>
</ul>

<h2>
<a id="io_concur" class="anchor" href="#io_concur" aria-hidden="true"><span class="octicon octicon-link"></span></a>Fibers and I/O Driven Concurrency
</h2>

<p>
The story here starts at linux's <b>epoll</b> api and how it supports efficient delivery of i/o
notification. In the <b>epoll</b> model an application can register a set of file descriptors
with the kernel, and with each one specify two pieces of data:

<ul>
<li>What i/o <em>events</em> it wants to be notified about</li>
<li>A pointer that it wants to be given when an i/o event occurs for that file descriptor</li>
</ul>

Linux uses file descriptors to represent lots of different things, but from a web services perspective
the most important and typical one is that network sockets are represented with file descriptors.  The
idea of an i/o <em>event</em> is something like the point in time when a socket becomes readable without blocking
-- that is, when data has arrived from the other computer such that a call to <b>recv</b> will return
something without blocking the calling thread.  That is the <b>EPOLLIN</b> event.
</p>

<p>
A reasonable picture of what is going on with <b>epoll</b> is here:
</p>

<p><img src="epoll.svg?raw=true" alt="Linux epoll picture"></p>

<p>
Because linux will store pointers for you associated with each file descriptor, and you can use these
pointers any way you want, like this example you can use them to store the addresses of C++ class
instances. These can implement the pieces of code you want to run whenever it is possible to read from,
or write to, a file <em>without blocking</em>.
</p>

<p>Consider the following pseudo-code:</p>

<div class="highlight highlight-C++"><pre>
<span class="c1">// runs forever...</span>
<span class="kt">void</span> <span class="nf">epoll_loop</span>()
{
  <span class="k">while</span> (<span class="kc">true</span>) {
    the_epoll_result = <span class="nf">epoll_wait</span>();
    the_object = (<span class="kc">base_class</span>*)the_epoll_result;
    the_object->handle_io();
  }
}
</pre></div>

<p>
<b>epoll_wait</b> is the linux api to the epoll system that waits until there is i/o possible on at
least one of the registered file descriptors, and when there is returns information about that file descriptor.
It isn't being shown correctly in this code example (the parameters are wrong), but in spirit it operates like
this example shows.  In this sort of model, if <b>epoll_wait</b> comes back with a 'result' that
represents file descriptor 48 in the picture above, then a simple virtual function call to <b>handle_io</b>
can get us to the code that knows what to do when <b>another.server.com</b> has sent information to us that we can
read without blocking.
</p>

</p>
In a number of ways linux's <b>epoll_wait</b> acts like both <b>poll</b> and <b>select</b>
except for a few important differences.  First, and most importantly, with epoll you don't have to send the
set of file descriptors you are interested in each time you call the function as you do with both poll
and select.  This means that the kernel doesn't have to read the set you are telling it on each call and
figure out what to do with each of them.  This can make a significant difference when you have a lot of file
descriptors that you are interested in - as is frequently the case for a web service that is under load.
It's also more convenient to let the kernel associate the file descriptors and pointers for you, than you
having to do it by hand.
</p>

<p>
A last important difference that is largely a consequence of the first one (not needing to specify
the file set on each call), is that it is now fairly straight forward to have multiple threads all waiting for
epoll i/o notification on the same file set.  This can further reduce the processing time needed to go from
event notification to your handling code in any case where you have multi-threaded event handling code.
In anon's epoll model, it calls <b>epoll_wait</b> on the same file set concurrently from multiple threads.
It then dispatches to the handling code directly using function calls as is shown in the code snippet
above.  These handlers simply execute in the thread that recieved the notification from <b>epoll_wait</b>.
</p>

<h3>
<a id="callback_design" class="anchor" href="#callback_deskgn" aria-hidden="true"><span class="octicon octicon-link"></span></a>Callback Oriented Design
</h3>

<p>
A significant drawback to this sort of epoll code is that while it is very efficient at notifying you
that you can read some data from a socket, it can't tell you whether you can read <em>all</em> the data
you might be interested in.  It could be that a server has responded with only some of the information it
is going to send you, and you are likely to need to start reading what it has sent so far to even determine
whether it has sent you a complete reply or not.
</p>

<p>
Consider the case where epoll tells you that you can read some data from the <b>another.server.com</b> socket and
you begin running your code to parse the http headers you expect to be at the start of this message.  It could
be that part way through parsing them you get to the end of the data that is currently available to read on that
socket. If this happens your parser can either use a <em>blocking</em> socket, in which case your next
<b>read</b> call on that socket will block the calling thread until the next network packet shows up from that
server, or you can use a <em>non-blocking</em> socket, and then detect the EAGAIN state.  But then you need to
write your http parser in such a way that it can be suspended and later resumed when the packet does show up
and you get the next epoll notification for it.
</p>

<p>
Each of these has drawbacks.  A common pattern found in code that attempts to always be <em>non-blocking</em>
is the one used by <b>libevent</b> and is the second of the two cases above.  This is sometimes called a
Callback Oriented Design because of its heavy use of callbacks.  The overall design features the idea that
i/o processing can take a long time, so you start by initiating it.  When you do this you provide a
callback function that will get called when the i/o operation completes.  But this sort of design
tends to interrupt the linear flow of source code because the idea of <em>what happens next</em> can't 
be easily specified as the next line in the source code.  In a Callback Oriented Design, the thing
that happens <em>next</em> is frequently specified in some other function.
</p>

<h3>
<a id="fibers" class="anchor" href="#fibers" aria-hidden="true"><span class="octicon octicon-link"></span></a>Fibers and how they help
</h3>

<p>
"Fibers" or "user level threads" is a concept that is supported by many different operation systems, including
linux.  The typical idea behind this is that they are similar to os threads.  They each have a stack and
register state.  You can make function calls, return from them and all the other "normal" things.  But they
requrire an explicit call to switch from one fiber to another.  This is unlike normal operating system threads which
the operating system switches for you on its own schedule.  For a whole host of reasons switching fiber contexts
by calling an API is almost always faster than the OS switching thread contexts.  But even though they are faster,
there frequently isn't a good way for your code to know when a good time would be to switch to some other
fiber.  Fibers tend to a tool that is good at only a very specific subset of the more general thread
concurrency problems.
</p>

<p>
But for anon, we have a perfect use case for fibers.  In anon, we can continue to execute in a single fiber
until that fiber runs out of i/o.  For example, a fiber can continue to execute and do whatever its logic
is designed for until it gets to the next point of wanting to read something from a socket when there is no
data to be read.  That EAGAIN condition can be used to signify an appropirate time to switch fibers since
some other fiber in the system might be at a point where there is data available for it to read or write.
</p>

<p>
A core piece of anon is that when a fiber hits an EAGAIN condition attempting to perform i/o, it does a
fiber switch back to the point where it calls <b>epoll_wait</b>.  This allows the threads that run these
fibers to remain busy executing interesting logic as long as there are sockets that are either ready to
be read from or written to.
</p>

<p>
Anon's fiber model permits code that is structured something like:
</p>

<div class="highlight highlight-C++"><pre>
<span class="c1">
// appears to run until the end of the message.
// but, in fact switches out to other fibers if
// this socket runs out of data and others have
// available data</span>
<span class="kt">void</span> <span class="nf">parse_message</span>()
{
  <span class="k">while</span> (token = <span class="nf">read_next_token</span>()) {
    <span class="nf">do_something_with</span>(token);
  }
}
</pre></div>

<p>
In anon, <b>read_next_token</b> does a fiber-friendly read, which means that as soon as there are no bytes
to be read it does a fiber-switch back to the <b>epoll_wait</b> code, alowing this
thread to work on whatever sockets do have available data.  When this, or any other thread sees that
there is again data for this socket (epoll_wait return a result for this socket), it fiber-switches back
to the point where this fiber previously switched out.  So from a coding perspective it looks like linear
flow the way that thread-blocking operations do.
</p>

<p>
This ends up meaning that the fiber schedule rules are based on epoll's i/o event delivery mechanism.
Anon implements an <b>i/o driven concurrency model.</b>
</p>

<h3>
<a id="fiber_timing" class="anchor" href="#fiber_timing" aria-hidden="true"><span class="octicon octicon-link"></span></a>Does that actually help?
</h3>

<p>
Like anything, it depends on what you measure against.  But you can do comparisons to similar thread-switching
by writing an application that does the following:
</p>

<ul>
<li>In one thread, <b>accept</b> socket connections and create "handlers" for each</li>
<li>In a second thread, make many <b>connect</b> calls to the listening socket of the first thread</li>
</ul>

<p>
Now, consider a very simple "protocol" where the sender (second thread) just sends a single byte.
When the handler sees a byte it echos that byte back.  After the sender sends its byte it
attempts to read this response byte from the handler.  To compare the fiber solution to a normal os
solution we want to look at the difference between the first thread creating an operating system
thread for each new handler vs. creating a fiber.  Since the syntax we can use once we are inside
of a fiber is similar to what we can do inside of a thread, we can just say that the handlers all
look something like:
</p>

<div class="highlight highlight-C++"><pre>
<span class="c1">
// protocol handler</span>
<span class="kt">void</span> <span class="nf">handle_message</span>(<span class="k">pipe</span>& p)
{
  <span class="k">while</span> (byte = p.<span class="nf">read</span>()) {
    p.<span class="nf">write</span>(byte);
  }
}
</pre></div>

<p>
In an os thread case <b>pipe</b> is a type where <b>read</b> and <b>write</b> are directly
mapped to <b>recv</b> and <b>send</b> and the underlying file descript is set <em>blocking</em>.
For the fiber case the file descriptor is <em>non-blocking</em> and the methods are mapped to versions of
these calls that check for <b>EAGAIN</b> and when they detect it, fiber-switch back to the
<b>epoll_wait</b> code.  In both cases, when the caller sends a zero byte the protocol is over and
the routine returns.
</p>

<p>
In the os thread case the code that calls <b>accept</b> looks something like:
</p>

<div class="highlight highlight-C++"><pre>
<span class="c1">
// accept loop</span>
<span class="kt">void</span> <span class="nf">accept_loop</span>(<span class="k">int</span> listening_socket)
{
  <span class="k">while</span> (<span class="kc">true</span>) {
    <span class="kt">int</span> new_connection = <span class="nf">accept</span>(listening_socket, <span class="m">0</span>, <span class="m">0</span>);
    std::thread(std::bind(<span class="nf">handle_message</span>, new_connection).<span class="nf">detach</span>();
  }
}
</pre></div>

<p>
where you (incorrectly) have to assume that there is some way to automatically convert the new_connection
int/file descriptor into a non-const <b>pipe</b>&.  But what is important here is to point out that each call
to <b>accept</b> results in a new os thread running <b>handle_message</b> with that one socket.
</p>

<p>
The fiber equivalent that we want to compare to is identical except that:
</p>

<ul>
<li>Its <b>handle_message</b> uses a <b>pipe</b> type that does fiber-friendly reads and writes</li>
<li>Its <b>accept_loop</b> creates fibers running handle_message instead of std::threads</li>
</ul>

        </section>

        <aside id="sidebar">
          <a href="https://github.com/pkholland/anon/zipball/master" class="button">
            <small>Download</small>
            .zip file
          </a>
          <a href="https://github.com/pkholland/anon/tarball/master" class="button">
            <small>Download</small>
            .tar.gz file
          </a>

          <p class="repo-owner"><a href="https://github.com/pkholland/anon"></a> is maintained by <a href="https://github.com/pkholland">pkholland</a>.</p>

          <p>This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the Architect theme by <a href="https://twitter.com/jasonlong">Jason Long</a>.</p>
        </aside>
      </div>
    </div>

  
  </body>
</html>

