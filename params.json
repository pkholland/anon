{"name":"Anon","tagline":"Experiments in Web Services Design","body":"Anon\r\n====\r\n\r\n![It Goes To 11!](https://cloud.githubusercontent.com/assets/8400193/4968088/496424a8-6832-11e4-8506-df449bf82b5c.png?raw=true)\r\n\r\n##The Anon Story\r\n\r\nAnon is an experiment in server design for _Services_.  It attempts to achieve\r\nseveral goals.  These are:\r\n\r\n* Maximize efficiency on a single CPU instance\r\n* Effeciently deal with \"over-maximum\" requests\r\n* Provide a reasonable dev environment\r\n\r\nAnon is highly experimental and is unlike other service designs in several ways.  For example, it is currently all written in C++.  The name \"anon\" comes from the first two goals, and is the word *anon*, meaning something like \"again\". It's not an abreviation for \"anonymous\".\r\n\r\nA common problem in some server designs is one that can be called the \"infinite queue\" problem.  It can be seen when the Service design has components that look something like the following -- shown in C++, but can exist in any language:\r\n\r\n```C++\r\n// some global\r\nstd::deque<int> g_new_connections;\r\n\r\n// runs in one thread\r\nvoid new_connections_loop(int listening_socket)\r\n{\r\n  while (true) {\r\n    int new_connection = accept(listening_socket, 0, 0);\r\n    g_new_connections.push_back(new_connection);\r\n  }\r\n}\r\n\r\n// runs in another thread\r\nvoid process_connections_loop()\r\n{\r\n  while (true) {\r\n    int conn = g_new_connections.front();\r\n    g_new_connections.pop_front();\r\n    process_one_connection(conn);\r\n  }\r\n}\r\n```\r\n\r\nThis code isn't meant to be fully correct.  It's missing mutex's and other stuff. But it shows a central feature of many Service designs where there exists some kind of queue, shown above as the std::deque `g_new_connections`.  Here one thread of execution accepts new server connections as fast as it can and puts each one on a queue.  Another thread pulls items off the of the queue and processes them as fast as it can.  The basic problem illustrated here is that there isn't a good way for the second thread to keep the first from getting too far ahead of it.  If the `process_one_connection` function takes a long time to complete relative to the speed at which other machines are trying to establish new connections to this one, then the `g_new_connections` queue can grow arbitrarily large.  The root cause of many Critical Service Outages, particularly when they occur due to excessive server load, can be traced to a basic problem where a consumer of queued requests falls behind the producer of them.  When that starts to happen the percentage of the Service's total compute and resource capacity that is dedicated to maintaining the queue itself grows.  In many\r\ndesigns this slows down `process_one_connection` more than it does `process_connections_loop`, which then componds the original problem.\r\n\r\nLinux has an errno code named EAGAIN which it uses when certain operations are set to be non-blocking and are not currently possible for one reason or another.  For example, trying to read from a non-blocking socket that\r\ndoesn't currently have any data to be read generates EAGAIN.  A common use of EAGAIN would be to set `listening_socket` above to be non-blocking.  Then the call to `accept` would return -1 and set errno to EAGAIN if there were not any connections that could be returned when it was called.  That kind of usage would allow the calling thread to go do something else instead of stay stuck in `accept` until someone tries to connect to the computer.\r\n\r\nBut EAGAIN can also be used on the write-side of an operation.  If `g_new_connections` were a pipe of some kind instead of the deque that is shown, then the `push_back` would be some kind of `write` call.  The `front`\r\nand `pop_front` would then be replaced by `read` calls.  In that kind of design the pipe could be set non-blocking, and since it has a finite internal size, if `new_connections_loop` gets too far in front of `process_connections_loop` the `write` call will fail with errno set to EAGAIN.  And this can serve as a natural way for a consumer of requests to signal to the producer that it needs to slow down.  The consumer doesn't need to do anything at all. The fact that it is unable to read fast enough causes the producer to be unable to continue writing.\r\n\r\nEven without setting the pipe to be non-blocking, using a pipe with a small-ish, finite capacity will cause `new_connections_loop` to *block* inside of its `write` call, which will keep it from being able to call `accept` again.  This would then keep client machines from being able to connect and send new requests.\r\nThat creates a kind of speed limit that `process_connections_loop` can assert on the entire Service.  But having client machines fail to connect without understanding why makes it hard to get those client machines\r\nworking correctly.  So a basic principle of Anon is to propogate the EAGAIN concept through the entire Service.\r\n\r\nIn the Anon design `new_connections_loop` does use a non-blocking pipe, and so sees that it has gotten too far ahead of `process_connections_loop`.  It can then enter a state where further `accept` calls are immediately replied with a kind of EAGAIN message and then shut down.  That distributes the EAGAIN processing throughout the entire service -- thus the name Anon for the project.\r\n\r\nConveniently, EAGAIN is errno 11, letting us also tie the name to the other goal of maximum efficiency.  A second piece of Anon is to provide a design that makes good use of Linux's event dispatching mechanism \"epoll\".  It provides a platform where all request processing can be done free of any blocking operations.  In fact, the goal is to allow Anon servers to run in a model where the number of os threads running is equal to the number of CPU cores.  In this model, each request is handled by user-level threads (fibers) and fiber scheduling is driven by Linux's epoll event dispatching mechanism.\r\n\r\n\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}